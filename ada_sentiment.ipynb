{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 250000 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n",
      "Processing chunk with 244449 rows\n",
      "This chunk does not contain missing values\n",
      "True True True True\n"
     ]
    }
   ],
   "source": [
    "#Searching for Missing Values\n",
    "def MissingValuesFilter_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "\n",
    "        #Gives True as a result, if the lines where NaN are presents are empty \n",
    "        #(meaning there are no NaNs):\n",
    "        missing_nan = pd.DataFrame(np.where(chunk.isnull().any(axis=1))).empty\n",
    "        #Gives True as a result if the lignes and corresponding columns where \n",
    "        #zeros, '' and None are found are empty:\n",
    "        missing_zeros = pd.DataFrame(np.where(chunk==0)).empty \n",
    "        missing_space = pd.DataFrame(np.where(chunk=='')).empty\n",
    "        missing_none = pd.DataFrame(np.where(chunk==None)).empty\n",
    "\n",
    "        print('This chunk does not contain missing values')\n",
    "        print(missing_nan, missing_zeros, missing_space, missing_none) \n",
    "\n",
    "PATH = './quotes-2020.json.bz2'\n",
    "\n",
    "for chunk in pd.read_json(PATH, lines=True, compression='bz2', chunksize=250000):\n",
    "    MissingValuesFilter_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 250000 rows\n",
      "Number of Occurences posible\n",
      "Processing chunk with 244449 rows\n",
      "Number of Occurences posible\n"
     ]
    }
   ],
   "source": [
    "#Checking if Number of Occurrences is coherent\n",
    "def NumOccurrencesFilter_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        size = len(chunk)\n",
    "        #To have a number of occurrences equal zero has no meaning, so here it checks if this data set has a logical \n",
    "        #number of occurrences\n",
    "        if chunk['numOccurrences'].all() == np.array(size*[0]).all():\n",
    "          print('Number of Occurrences impossible')\n",
    "        else :\n",
    "          print('Number of Occurences possible')\n",
    "\n",
    "PATH = './quotes-2020.json.bz2'\n",
    "\n",
    "for chunk in pd.read_json(PATH, lines=True, compression='bz2', chunksize=250000):\n",
    "    NumOccurrencesFilter_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 250000 rows\n",
      "Probability of Autors higher than 50%\n",
      "Processing chunk with 244449 rows\n",
      "Probability of Autors higher than 50%\n"
     ]
    }
   ],
   "source": [
    "#Checking the probabilities of the autors\n",
    "def ProbasFilter_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        size = len(chunk)\n",
    "        #Here it is checked that the first probability for the autor is higher than 50%\n",
    "        #If this probability is lower, we will chose to discard the corresponding quotation because we consider that \n",
    "        #we are not sure enough of who is the autor of the quotation\n",
    "        if chunk['probas'].str[0].str[1].astype(float).all() < np.array(size*[0.5]).all():\n",
    "          print('Probability of Autors of quotation too low')\n",
    "        else :\n",
    "          print('Probability of Autors higher than 50%')\n",
    "\n",
    "PATH = './quotes-2020.json.bz2'\n",
    "\n",
    "for chunk in pd.read_json(PATH, lines=True, compression='bz2', chunksize=250000):\n",
    "    ProbasFilter_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 250000 rows\n",
      "The Autor has a high probability of having generated the quotation\n",
      "Processing chunk with 244449 rows\n",
      "The Autor has a high probability of having generated the quotation\n"
     ]
    }
   ],
   "source": [
    "#Checking if the autor is the one with the highest probability of having generated the quotation\n",
    "def ProbasFilter_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "  \n",
    "        if chunk['speaker'].all() != chunk['probas'].str[0].str[0].all():\n",
    "          print('The Autor does not correspond to the high probability Autor')\n",
    "        else :\n",
    "          print('The Autor has a high probability of having generated the quotation')\n",
    "\n",
    "PATH = './quotes-2020.json.bz2'\n",
    "\n",
    "for chunk in pd.read_json(PATH, lines=True, compression='bz2', chunksize=250000):\n",
    "    ProbasFilter_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the Data with Streaming \n",
    "\n",
    "#THIS PART IS ONLY NEEDED IF THE TESTS FROM BEFORE SHOWED PROBLEMS\n",
    "#This is not the case for the year 2020 that was checked here, but could be for the other years\n",
    "\n",
    "#If the above checks show that there is a missing values/incoherence or problem in the data for any year, \n",
    "#this code could remove the above mentionned problems and creates a new file with the filtered data, that could be used \n",
    "#for the rest of the project\n",
    "\n",
    "path_to_file = './quotes-2020.json.bz2' \n",
    "path_to_out = './quotes-2020-filtered.json.bz2'\n",
    "\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            #Removes '', None, 0 and False from the quotation in instance:\n",
    "            if any([bool(instance['quotation']) == True for quotation in instance]): \n",
    "              #If the probability the quotation comes from a speaker is lower \n",
    "              #than 50% than we discard this probability (as not high enough):\n",
    "              if float(instance['probas'][0][1]) > 0.5: \n",
    "                #Removes the lines that are not coherents with respect to the speakers probabilities\n",
    "                if instance['speaker'] == instance['probas'][0][0]: \n",
    "                  if instance['numOccurrences'] != 0: #Removes incoherent occurences\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feasability of the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the useful DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Streaming to create a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of a new file with a new column for the domain (cf colab)\n",
    "from tld import get_tld\n",
    "\n",
    "def get_domain(url):\n",
    "    res = get_tld(url, as_object=True)\n",
    "    return res.tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of a file with additional columns: the domain and the name of the intenet website\n",
    "path_to_file = 'quotes-2016.json.bz2' \n",
    "path_to_out = '2016a.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            urls = instance['urls'] # extracting list of links\n",
    "            domains = []\n",
    "            website = []\n",
    "            for url in urls:\n",
    "                tld = get_domain(url)\n",
    "                domains.append(tld)\n",
    "                net = urlparse(url)\n",
    "                neto=net.netloc\n",
    "                website.append(neto)\n",
    "            instance['domains'] = domains # updating the sample with domain name\n",
    "            instance['website'] = website\n",
    "            d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Cleanning and Storage of the Data for One Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 0 duplicated rows\n",
      "(4538, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4590, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4473, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4520, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4754, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4642, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4508, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4665, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4530, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4586, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4617, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4428, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4680, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4736, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4522, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4546, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4514, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4544, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4678, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4654, 4)\n",
      "Processing chunk with 250000 rows\n",
      "There were 0 duplicated rows\n",
      "(4446, 4)\n",
      "Processing chunk with 244449 rows\n"
     ]
    }
   ],
   "source": [
    "#Cleaning of the 2020 Data\n",
    "df = pd.DataFrame() #Defining the DataFrame that will contain the wanted chucks\n",
    "\n",
    "def process_chunk(chunk, df):\n",
    "        df_1 = pd.DataFrame() #Initialising intermediate DataFrame\n",
    "        df_2 = pd.DataFrame() #Initialising intermediate DataFrame\n",
    "\n",
    "        # Calculates duplicated rows in the chunk\n",
    "        num_duplicates = len(chunk[chunk.duplicated(subset=[\"quoteID\"])])\n",
    "        print(\"There were {} duplicated rows\".format(num_duplicates))\n",
    "\n",
    "        # Removes duplicated rows\n",
    "        df = chunk.drop_duplicates(subset=[\"quoteID\"], keep=\"first\")\n",
    "\n",
    "        df = chunk.drop(['quoteID', 'qids', 'numOccurrences', 'probas', 'urls', 'phase', 'domains'], axis=1) #axis=1 for columns\n",
    "        \n",
    "        df['website'] = df['website'].astype('str') #Converting to string to be able to use str.contains \n",
    "        df_1 = df[df['website'].str.contains('foxnews')] #Creating a DataFrame containing only foxnews\n",
    "        df_2 = df[df['website'].str.contains('nytimes')] #Creating a DataFrame containing only nytimes\n",
    "        df = pd.concat([df_1, df_2])\n",
    "        #Some speakers had their names not starting with a capslocks. This line of code corrects it:\n",
    "        df[\"speaker\"] = df[\"speaker\"].apply(str.title)\n",
    "\n",
    "        print(df.shape)\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        return df\n",
    "        \n",
    "\n",
    "for chunk in pd.read_json('./2020a.json.bz2', lines=True, compression='bz2', chunksize=250000):\n",
    "    df = process_chunk(chunk, df)\n",
    "    with open('ReadyToUseData.pickle', 'ab') as file: #Opening pickle file in append mode, to keep the previous data of the file\n",
    "        pickle.dump(df, file) #Adding the new chunks to the pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Handling the entire Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a DataFrame is created over the 5 years period, with the quotations containing a specific word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File finished\n",
      "Folder finished\n"
     ]
    }
   ],
   "source": [
    "#Here \"immigrat\" is chosen as the key word\n",
    "path = os.getcwd()\n",
    "json_files = glob.glob(os.path.join(path, \"*.json.bz2\"))\n",
    "lista='quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences','probas', 'urls', 'phase', 'domains', 'website'\n",
    "df2=pd.DataFrame(columns=lista) \n",
    "\n",
    "def process_chunk(chunk,df2):\n",
    "        immi=pd.DataFrame()\n",
    "        immi=chunk[chunk['quotation'].str.contains(\"immigrat\")]\n",
    "        df2 = df2.append(immi)\n",
    "        return df2\n",
    "    \n",
    "for f in json_files:      \n",
    "    for chunk in pd.read_json(f, lines=True, compression='bz2', chunksize=100000, encoding='utf-8'):\n",
    "        df2=process_chunk(chunk,df2)\n",
    "    print('File finished')\n",
    "print('Folder finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 0 duplicated rows\n"
     ]
    }
   ],
   "source": [
    "#Dropping dupliactes, if any\n",
    "num_duplicates2 = len(df2[df2.duplicated(subset=[\"quoteID\"])])\n",
    "print(\"There were {} duplicated rows\".format(num_duplicates2))\n",
    "duplica2 = df2.drop_duplicates(subset=[\"quoteID\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File finished\n",
      "Folder finished\n"
     ]
    }
   ],
   "source": [
    "#Creation of the dataframe from the 5 years with the citations of a particular website\n",
    "path = os.getcwd()\n",
    "json_files = glob.glob(os.path.join(path, \"*.json.bz2\"))\n",
    "lista='quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences','probas', 'urls', 'phase', 'domains', 'website'\n",
    "df3=pd.DataFrame(columns=lista) \n",
    "\n",
    "def process_chunk(chunk,df3):\n",
    "        chunk['website']=chunk['website'].astype('str')\n",
    "        journal=pd.DataFrame()\n",
    "        journal=chunk[chunk['website'].str.contains(\"www.nytimes.com\")]\n",
    "        df3 = df3.append(journal)\n",
    "        return df3\n",
    "    \n",
    "for f in json_files:      \n",
    "    for chunk in pd.read_json(f, lines=True, compression='bz2', chunksize=25000, encoding='utf-8'):\n",
    "        df3=process_chunk(chunk,df3)\n",
    "    print('File finished')\n",
    "print('Folder finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "      <th>domains</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2016-09-28-002644</td>\n",
       "      <td>a more free hand in implementing their hard-li...</td>\n",
       "      <td>Benjamin Netanyahu</td>\n",
       "      <td>[Q43723]</td>\n",
       "      <td>2016-09-28 00:00:00</td>\n",
       "      <td>66</td>\n",
       "      <td>[[Benjamin Netanyahu, 0.7465], [None, 0.2044],...</td>\n",
       "      <td>[http://www.timesofisrael.com/the-old-middle-e...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com, com, com, com, com, com, com, com, com, ...</td>\n",
       "      <td>['www.timesofisrael.com', 'www.sfgate.com', 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1716</td>\n",
       "      <td>2016-09-14-038466</td>\n",
       "      <td>I do not feel safe at a company that tolerates...</td>\n",
       "      <td>Tim Cook</td>\n",
       "      <td>[Q1404825, Q265852, Q7803347, Q7803348]</td>\n",
       "      <td>2016-09-14 23:00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>[[Tim Cook, 0.6259], [None, 0.3723], [Han Guan...</td>\n",
       "      <td>[http://www.refinery29.com/2016/09/123254/appl...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com, com.au, co.nz, com, com, com, com, com, ...</td>\n",
       "      <td>['www.refinery29.com', 'www.perthnow.com.au', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1781</td>\n",
       "      <td>2016-09-25-022013</td>\n",
       "      <td>I encourage Donald Trump and his supporters to...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-09-25 19:59:00</td>\n",
       "      <td>51</td>\n",
       "      <td>[[None, 0.9392], [Ahmad Khan Rahami, 0.0593], ...</td>\n",
       "      <td>[http://www.philly.com/philly/news/new_jersey/...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com, com, com, com, com, com, com, com, com, ...</td>\n",
       "      <td>['www.philly.com', 'mobile.sfgate.com', 'www.s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2377</td>\n",
       "      <td>2016-02-15-039900</td>\n",
       "      <td>I wish I knew for sure if they could have had ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2016-02-15 00:00:00</td>\n",
       "      <td>97</td>\n",
       "      <td>[[None, 0.6376], [Jesse Hughes, 0.1823], [Davi...</td>\n",
       "      <td>[http://dailyherald.com/article/20160215/entli...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com, com, com, com, com, com, com, com, com, ...</td>\n",
       "      <td>['dailyherald.com', 'www.usnews.com', 'tdn.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2417</td>\n",
       "      <td>2016-02-18-053756</td>\n",
       "      <td>I'd just say that this man is not Christian, i...</td>\n",
       "      <td>Pope Francis</td>\n",
       "      <td>[Q450675]</td>\n",
       "      <td>2016-02-18 00:00:00</td>\n",
       "      <td>477</td>\n",
       "      <td>[[Pope Francis, 0.8153], [None, 0.1265], [Dona...</td>\n",
       "      <td>[http://dailyherald.com/article/20160218/news/...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com, com, com, com, com, com, com, com, com, ...</td>\n",
       "      <td>['dailyherald.com', 'www.deseretnews.com', 'ww...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                quoteID                                          quotation  \\\n",
       "84    2016-09-28-002644  a more free hand in implementing their hard-li...   \n",
       "1716  2016-09-14-038466  I do not feel safe at a company that tolerates...   \n",
       "1781  2016-09-25-022013  I encourage Donald Trump and his supporters to...   \n",
       "2377  2016-02-15-039900  I wish I knew for sure if they could have had ...   \n",
       "2417  2016-02-18-053756  I'd just say that this man is not Christian, i...   \n",
       "\n",
       "                 speaker                                     qids  \\\n",
       "84    Benjamin Netanyahu                                 [Q43723]   \n",
       "1716            Tim Cook  [Q1404825, Q265852, Q7803347, Q7803348]   \n",
       "1781                None                                       []   \n",
       "2377                None                                       []   \n",
       "2417        Pope Francis                                [Q450675]   \n",
       "\n",
       "                    date numOccurrences  \\\n",
       "84   2016-09-28 00:00:00             66   \n",
       "1716 2016-09-14 23:00:00             11   \n",
       "1781 2016-09-25 19:59:00             51   \n",
       "2377 2016-02-15 00:00:00             97   \n",
       "2417 2016-02-18 00:00:00            477   \n",
       "\n",
       "                                                 probas  \\\n",
       "84    [[Benjamin Netanyahu, 0.7465], [None, 0.2044],...   \n",
       "1716  [[Tim Cook, 0.6259], [None, 0.3723], [Han Guan...   \n",
       "1781  [[None, 0.9392], [Ahmad Khan Rahami, 0.0593], ...   \n",
       "2377  [[None, 0.6376], [Jesse Hughes, 0.1823], [Davi...   \n",
       "2417  [[Pope Francis, 0.8153], [None, 0.1265], [Dona...   \n",
       "\n",
       "                                                   urls phase  \\\n",
       "84    [http://www.timesofisrael.com/the-old-middle-e...     E   \n",
       "1716  [http://www.refinery29.com/2016/09/123254/appl...     E   \n",
       "1781  [http://www.philly.com/philly/news/new_jersey/...     E   \n",
       "2377  [http://dailyherald.com/article/20160215/entli...     E   \n",
       "2417  [http://dailyherald.com/article/20160218/news/...     E   \n",
       "\n",
       "                                                domains  \\\n",
       "84    [com, com, com, com, com, com, com, com, com, ...   \n",
       "1716  [com, com.au, co.nz, com, com, com, com, com, ...   \n",
       "1781  [com, com, com, com, com, com, com, com, com, ...   \n",
       "2377  [com, com, com, com, com, com, com, com, com, ...   \n",
       "2417  [com, com, com, com, com, com, com, com, com, ...   \n",
       "\n",
       "                                                website  \n",
       "84    ['www.timesofisrael.com', 'www.sfgate.com', 'w...  \n",
       "1716  ['www.refinery29.com', 'www.perthnow.com.au', ...  \n",
       "1781  ['www.philly.com', 'mobile.sfgate.com', 'www.s...  \n",
       "2377  ['dailyherald.com', 'www.usnews.com', 'tdn.com...  \n",
       "2417  ['dailyherald.com', 'www.deseretnews.com', 'ww...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 0 duplicated rows\n"
     ]
    }
   ],
   "source": [
    "#Dropping dupliactes, if any\n",
    "num_duplicates3 = len(df3[df3.duplicated(subset=[\"quoteID\"])])\n",
    "print(\"There were {} duplicated rows\".format(num_duplicates3))\n",
    "duplica3 = df3.drop_duplicates(subset=[\"quoteID\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[www.breitbart.com]</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[www.foxnews.com]</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[www.msn.com]</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[www.politico.com]</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[www.independent.co.uk]</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         website\n",
       "[www.breitbart.com]           67\n",
       "[www.foxnews.com]             28\n",
       "[www.msn.com]                 25\n",
       "[www.politico.com]            22\n",
       "[www.independent.co.uk]       21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking that the column was well added\n",
    "lis=df2['website'].value_counts().to_frame()\n",
    "lis.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>10923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>President Trump</th>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joe Biden</th>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernie Sanders</th>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>President Donald Trump</th>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        speaker\n",
       "None                      10923\n",
       "President Trump             779\n",
       "Joe Biden                   489\n",
       "Bernie Sanders              441\n",
       "President Donald Trump      414"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualisation of higher quoted speakers\n",
    "lis2=df3['speaker'].value_counts().to_frame()\n",
    "lis2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the website column type to string\n",
    "df2['website']=df2['website'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of DataFrames for some journals\n",
    "fox=df2[df2['website'].str.contains(\"www.foxnews.com\")]\n",
    "ny=df2[df2['website'].str.contains(\"www.nytimes.com\")]\n",
    "brei=df2[df2['website'].str.contains(\"www.breitbart.com\")]\n",
    "cnn=df2[df2['website'].str.contains(\"cnn.com\")]\n",
    "guard=df2[df2['website'].str.contains(\"www.theguardian.com\")]\n",
    "slate=df2[df2['website'].str.contains(\"slate.com\")]\n",
    "buzz=df2[df2['website'].str.contains(\"buzzfeed.com\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_quotes=fox['quotation']\n",
    "ny_quotes=ny['quotation']\n",
    "brei_quotes=brei['quotation']\n",
    "cnn_quotes=cnn['quotation']\n",
    "guard_quotes=guard['quotation']\n",
    "slate_quotes=slate['quotation']\n",
    "buzz_quotes=buzz['quotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_length=len(fox)\n",
    "ny_length=len(ny)\n",
    "brei_length=len(brei)\n",
    "cnn_length=len(cnn)\n",
    "guard_length=len(guard)\n",
    "slate=len(slate)\n",
    "buzz_length=len(buzz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of positif and negatif feelings found respectively are: 0.11661666666666663 0.08851666666666672\n"
     ]
    }
   ],
   "source": [
    "#Sentimental Analysis for positif and negatif feelings for the previously chosen articles with df2\n",
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in fox_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos+=result[\"pos\"]\n",
    "    neg+=result[\"neg\"]\n",
    "average_pos=pos/fox_length\n",
    "average_neg=neg/fox_length\n",
    "print('Mean of positif and negatif feelings found respectively are:', average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of positif and negatif feelings found respectively are: 0.12173076923076925 0.0723846153846154\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in ny_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos+=result[\"pos\"]\n",
    "    neg+=result[\"neg\"]\n",
    "average_pos=pos/ny_length\n",
    "average_neg=neg/ny_length\n",
    "print('Mean of positif and negatif feelings found respectively are:', average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the results of the sentimental analysis on both medias (Foxnews and New York Times), we can see that the New York Times is more positive about immigration while Foxnews is more negative. Nevertheless, one should check if these differences are significative or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_quotes=df3['quotation']\n",
    "df3size=len(df3_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of positif and negatif feelings found respectively 0.11050858949963867 0.0794943585345589\n"
     ]
    }
   ],
   "source": [
    "#Sentimental Analysis for positif and negatif feelings for the previously chosen articles with df3\n",
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in df3_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/df3size\n",
    "average_neg=neg/df3size\n",
    "print('Mean of positif and negatif feelings found respectively',average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "      <th>domains</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>2020-04-09-028183</td>\n",
       "      <td>in the pocket of China</td>\n",
       "      <td>Tom Cotton</td>\n",
       "      <td>[Q3090307]</td>\n",
       "      <td>2020-04-09 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Tom Cotton, 0.7595], [None, 0.2075], [Tedros...</td>\n",
       "      <td>[http://www.foxnews.com/media/sen-tom-cotton-w...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com, com]</td>\n",
       "      <td>['www.foxnews.com', 'www.newsmax.com']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>2020-04-10-024002</td>\n",
       "      <td>It doesn't appear salvageable,</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-04-10 00:00:00</td>\n",
       "      <td>29</td>\n",
       "      <td>[[None, 0.6247], [Donovan Mitchell, 0.1627], [...</td>\n",
       "      <td>[http://www.foxnews.com/sports/utah-jazz-rudy-...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com, com, com, com, com, com, com, com.au, co...</td>\n",
       "      <td>['www.foxnews.com', 'www.sportsoutwest.com', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>2020-04-08-052001</td>\n",
       "      <td>The Pacific commander, Admiral Davidson, order...</td>\n",
       "      <td>Kirk Lippold</td>\n",
       "      <td>[Q6415477]</td>\n",
       "      <td>2020-04-08 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Kirk Lippold, 0.8545], [None, 0.1455]]</td>\n",
       "      <td>[http://www.foxnews.com/media/ex-uss-cole-comm...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com]</td>\n",
       "      <td>['www.foxnews.com']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>2020-03-30-008233</td>\n",
       "      <td>Certainly, he has a lot of great qualities tha...</td>\n",
       "      <td>Matthew Slater</td>\n",
       "      <td>[Q52906896, Q6791235]</td>\n",
       "      <td>2020-03-30 21:15:42</td>\n",
       "      <td>5</td>\n",
       "      <td>[[Matthew Slater, 0.5999], [None, 0.3473], [Ja...</td>\n",
       "      <td>[https://www.bostonherald.com/2020/03/30/patri...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com, com, com, com, com]</td>\n",
       "      <td>['www.bostonherald.com', 'www.foxnews.com', 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4966</th>\n",
       "      <td>2020-03-25-093901</td>\n",
       "      <td>While withholding the notes and many other exa...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2020-03-25 00:00:00</td>\n",
       "      <td>43</td>\n",
       "      <td>[[None, 0.9002], [Lori Loughlin, 0.0828], [Wil...</td>\n",
       "      <td>[http://kaaltv.com/national-news/loughlin-gian...</td>\n",
       "      <td>E</td>\n",
       "      <td>[com, com, com, com, com, com, com, com, com, ...</td>\n",
       "      <td>['kaaltv.com', 'kstp.com', 'www.nbcchicago.com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                quoteID                                          quotation  \\\n",
       "981   2020-04-09-028183                             in the pocket of China   \n",
       "1010  2020-04-10-024002                     It doesn't appear salvageable,   \n",
       "1771  2020-04-08-052001  The Pacific commander, Admiral Davidson, order...   \n",
       "2829  2020-03-30-008233  Certainly, he has a lot of great qualities tha...   \n",
       "4966  2020-03-25-093901  While withholding the notes and many other exa...   \n",
       "\n",
       "             speaker                   qids                date  \\\n",
       "981       Tom Cotton             [Q3090307] 2020-04-09 00:00:00   \n",
       "1010            None                     [] 2020-04-10 00:00:00   \n",
       "1771    Kirk Lippold             [Q6415477] 2020-04-08 00:00:00   \n",
       "2829  Matthew Slater  [Q52906896, Q6791235] 2020-03-30 21:15:42   \n",
       "4966            None                     [] 2020-03-25 00:00:00   \n",
       "\n",
       "     numOccurrences                                             probas  \\\n",
       "981               2  [[Tom Cotton, 0.7595], [None, 0.2075], [Tedros...   \n",
       "1010             29  [[None, 0.6247], [Donovan Mitchell, 0.1627], [...   \n",
       "1771              1           [[Kirk Lippold, 0.8545], [None, 0.1455]]   \n",
       "2829              5  [[Matthew Slater, 0.5999], [None, 0.3473], [Ja...   \n",
       "4966             43  [[None, 0.9002], [Lori Loughlin, 0.0828], [Wil...   \n",
       "\n",
       "                                                   urls phase  \\\n",
       "981   [http://www.foxnews.com/media/sen-tom-cotton-w...     E   \n",
       "1010  [http://www.foxnews.com/sports/utah-jazz-rudy-...     E   \n",
       "1771  [http://www.foxnews.com/media/ex-uss-cole-comm...     E   \n",
       "2829  [https://www.bostonherald.com/2020/03/30/patri...     E   \n",
       "4966  [http://kaaltv.com/national-news/loughlin-gian...     E   \n",
       "\n",
       "                                                domains  \\\n",
       "981                                          [com, com]   \n",
       "1010  [com, com, com, com, com, com, com, com.au, co...   \n",
       "1771                                              [com]   \n",
       "2829                          [com, com, com, com, com]   \n",
       "4966  [com, com, com, com, com, com, com, com, com, ...   \n",
       "\n",
       "                                                website  \n",
       "981              ['www.foxnews.com', 'www.newsmax.com']  \n",
       "1010  ['www.foxnews.com', 'www.sportsoutwest.com', '...  \n",
       "1771                                ['www.foxnews.com']  \n",
       "2829  ['www.bostonherald.com', 'www.foxnews.com', 'w...  \n",
       "4966  ['kaaltv.com', 'kstp.com', 'www.nbcchicago.com...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creation of a mask to analyse all the quotations from a certain time period. Here, we use the dataframe with foxnews's quotations\n",
    "mask = (df3['date'] > '2020-03-19') & (df3['date'] <= '2020-04-30')\n",
    "df_time=df3.loc[mask]\n",
    "df_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most commom words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('going', 3202),\n",
       " ('people', 3025),\n",
       " ('about', 2788),\n",
       " ('think', 2640),\n",
       " ('their', 2399),\n",
       " (\"don't\", 2050),\n",
       " ('would', 2019),\n",
       " ('because', 1766),\n",
       " ('there', 1558),\n",
       " ('really', 1457),\n",
       " ('other', 1214),\n",
       " ('Trump', 1198),\n",
       " ('these', 1195),\n",
       " ('right', 1185),\n",
       " ('those', 1151),\n",
       " ('should', 1139),\n",
       " (\"that's\", 1122),\n",
       " (\"we're\", 1094),\n",
       " ('could', 1080),\n",
       " ('which', 1023),\n",
       " ('where', 998),\n",
       " ('being', 925),\n",
       " ('president', 922),\n",
       " ('every', 856),\n",
       " ('President', 813),\n",
       " ('health', 810),\n",
       " ('things', 792),\n",
       " ('never', 779),\n",
       " ('American', 749),\n",
       " ('doing', 749),\n",
       " ('first', 746),\n",
       " (\"didn't\", 732),\n",
       " ('through', 707),\n",
       " (\"you're\", 689),\n",
       " (\"can't\", 675),\n",
       " ('something', 666),\n",
       " (\"That's\", 655),\n",
       " ('still', 651),\n",
       " (\"they're\", 648),\n",
       " ('There', 633),\n",
       " ('after', 629),\n",
       " ('great', 614),\n",
       " ('public', 611),\n",
       " ('Biden', 610),\n",
       " ('against', 604),\n",
       " ('country', 594),\n",
       " ('thing', 591),\n",
       " (\"We're\", 590),\n",
       " ('that,', 572),\n",
       " ('Democratic', 564),\n",
       " ('years', 555),\n",
       " ('Bernie', 551),\n",
       " ('always', 544),\n",
       " ('trying', 534),\n",
       " ('believe', 518),\n",
       " ('getting', 512),\n",
       " ('little', 512),\n",
       " ('Democrats', 510),\n",
       " ('United', 503),\n",
       " ('before', 493),\n",
       " ('Sanders', 493),\n",
       " ('support', 491),\n",
       " ('around', 490),\n",
       " ('coronavirus', 487),\n",
       " ('working', 484),\n",
       " ('continue', 483),\n",
       " (\"doesn't\", 481),\n",
       " (\"there's\", 464),\n",
       " ('world', 459),\n",
       " ('actually', 453),\n",
       " ('that.', 448),\n",
       " ('House', 445),\n",
       " ('better', 443),\n",
       " ('know,', 442),\n",
       " ('state', 429),\n",
       " ('Donald', 427),\n",
       " ('important', 426),\n",
       " ('while', 420),\n",
       " ('during', 417),\n",
       " ('government', 413),\n",
       " ('campaign', 406),\n",
       " ('virus', 390),\n",
       " ('thought', 389),\n",
       " ('family', 387),\n",
       " ('time,', 387),\n",
       " ('coming', 387),\n",
       " ('making', 386),\n",
       " ('everyone', 384),\n",
       " ('political', 383),\n",
       " ('different', 383),\n",
       " ('everything', 376),\n",
       " (\"we've\", 361),\n",
       " ('looking', 357),\n",
       " ('taking', 353),\n",
       " ('Americans', 352),\n",
       " ('under', 347),\n",
       " (\"There's\", 345),\n",
       " ('Senate', 343),\n",
       " ('start', 337),\n",
       " ('another', 335)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most common words used in each newspaper for the year 2020\n",
    "from collections import Counter\n",
    "Counter(\" \".join(df3[\"quotation\"]).split()).most_common(100)\n",
    "output = [' '.join([word\n",
    "   for word in sentence.split() if len(word) > 4\n",
    "]) for sentence in df3[\"quotation\"]]\n",
    "Counter(\" \".join(output).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here one can see that the most used words are connectors or verbs. The next step would be to print only the most occuring nouns or proper nouns. This could be done using NLTK library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text2emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Franois\n",
      "[nltk_data]     CHARROIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Franois\n",
      "[nltk_data]     CHARROIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Franois\n",
      "[nltk_data]     CHARROIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import text2emotion as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"britania rules the waves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "di=te.get_emotion(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Happy': 0.0, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(di.get('Fear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_quotes=df3['quotation']\n",
    "df3size=len(df3_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear0.3540972315278878, happy0.10009138906010918, angry0.047276552408007856, surprise0.20230884678261693, sad0.20133170744779985\n"
     ]
    }
   ],
   "source": [
    "fear=0\n",
    "happy=0\n",
    "angry=0\n",
    "surprise=0\n",
    "sad=0\n",
    "average_fear=0\n",
    "average_happy=0\n",
    "average_angry=0\n",
    "average_surprise=0\n",
    "average_sad=0\n",
    "\n",
    "for quotation in df3_quotes:\n",
    "    di=te.get_emotion(quotation)\n",
    "    fear+=di.get('Fear')\n",
    "    happy+=di.get('Happy')\n",
    "    angry+=di.get('Angry')\n",
    "    surprise+=di.get('Surprise')\n",
    "    sad+=di.get('Sad')\n",
    "average_fear=fear/df3size\n",
    "average_happy=happy/df3size\n",
    "average_angry=angry/df3size\n",
    "average_surprise=surprise/df3size\n",
    "average_sad=sad/df3size\n",
    "\n",
    "print(\"fear{}, happy{}, angry{}, surprise{}, sad{}\".format(average_fear,average_happy,\n",
    "                                                           average_angry,average_surprise,average_sad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear0.3395064569308952, happy0.09383088336309779, angry0.0476741310619592, surprise0.17941441132023217, sad0.1946043412556625\n"
     ]
    }
   ],
   "source": [
    "df3_quotes=df3['quotation']\n",
    "df3size=len(df3_quotes)\n",
    "fear=0\n",
    "happy=0\n",
    "angry=0\n",
    "surprise=0\n",
    "sad=0\n",
    "average_fear=0\n",
    "average_happy=0\n",
    "average_angry=0\n",
    "average_surprise=0\n",
    "average_sad=0\n",
    "\n",
    "for quotation in df3_quotes:\n",
    "    di=te.get_emotion(quotation)\n",
    "    fear+=di.get('Fear')\n",
    "    happy+=di.get('Happy')\n",
    "    angry+=di.get('Angry')\n",
    "    surprise+=di.get('Surprise')\n",
    "    sad+=di.get('Sad')\n",
    "average_fear=fear/df3size\n",
    "average_happy=happy/df3size\n",
    "average_angry=angry/df3size\n",
    "average_surprise=surprise/df3size\n",
    "average_sad=sad/df3size\n",
    "\n",
    "print(\"fear{}, happy{}, angry{}, surprise{}, sad{}\".format(average_fear,average_happy,\n",
    "                                                           average_angry,average_surprise,average_sad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction mot par journal et fonction analyse nltk et text2emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of positif and negatif feelings found respectively are: 0.11151355206847345 0.08521255349500706\n",
      "Mean of positif and negatif feelings found respectively are: 0.10453934191702435 0.08705007153075824\n",
      "Mean of positif and negatif feelings found respectively are: 0.1023164893617021 0.0847393617021276\n",
      "Mean of positif and negatif feelings found respectively are: 0.11235488505747131 0.07838362068965515\n",
      "Mean of positif and negatif feelings found respectively are: 0.11658677685950403 0.07373278236914602\n",
      "Mean of positif and negatif feelings found respectively are: 0.11021194879089617 0.08059459459459463\n",
      "Mean of positif and negatif feelings found respectively are: 0.09626578560939793 0.07705286343612341\n",
      "Mean of positif and negatif feelings found respectively are: 0.11254598540145996 0.07633284671532845\n",
      "Mean of positif and negatif feelings found respectively are: 0.11365738161559899 0.07723537604456825\n",
      "Mean of positif and negatif feelings found respectively are: 0.10537749287749289 0.08212678062678054\n",
      "Mean of positif and negatif feelings found respectively are: 0.10703875968992242 0.07958139534883724\n",
      "Mean of positif and negatif feelings found respectively are: 0.1096353436185134 0.07645722300140247\n",
      "Mean of positif and negatif feelings found respectively are: 0.10984480431848859 0.08238731443994604\n",
      "Mean of positif and negatif feelings found respectively are: 0.11379999999999994 0.08089523809523815\n",
      "Mean of positif and negatif feelings found respectively are: 0.11181818181818173 0.07365767045454552\n",
      "Mean of positif and negatif feelings found respectively are: 0.11219174434087877 0.07631291611185093\n",
      "Mean of positif and negatif feelings found respectively are: 0.10647901591895792 0.08500868306801733\n",
      "Mean of positif and negatif feelings found respectively are: 0.10539889196675895 0.0850193905817174\n",
      "Mean of positif and negatif feelings found respectively are: 0.11565079365079371 0.07486147186147193\n",
      "Mean of positif and negatif feelings found respectively are: 0.11743937418513689 0.07389830508474575\n",
      "Mean of positif and negatif feelings found respectively are: 0.10464110429447851 0.08016411042944793\n",
      "Mean of positif and negatif feelings found respectively are: 0.10494812680115279 0.08104899135446686\n",
      "Mean of positif and negatif feelings found respectively are: 0.11300430416068864 0.08056671449067426\n",
      "Mean of positif and negatif feelings found respectively are: 0.11161079545454555 0.08417045454545455\n",
      "Mean of positif and negatif feelings found respectively are: 0.1147237410071942 0.07845467625899279\n",
      "Mean of positif and negatif feelings found respectively are: 0.10836612021857928 0.07701366120218582\n",
      "Mean of positif and negatif feelings found respectively are: 0.10883404255319139 0.08075602836879424\n",
      "Mean of positif and negatif feelings found respectively are: 0.11443484419263443 0.08063031161473087\n",
      "Mean of positif and negatif feelings found respectively are: 0.1097518573551263 0.07649628528974738\n",
      "Mean of positif and negatif feelings found respectively are: 0.10819111111111113 0.07902814814814814\n",
      "Mean of positif and negatif feelings found respectively are: 0.11497982708933713 0.07793083573487028\n",
      "Mean of positif and negatif feelings found respectively are: 0.10107296137339059 0.08566380543633757\n",
      "Mean of positif and negatif feelings found respectively are: 0.11011246612466119 0.08393766937669374\n",
      "Mean of positif and negatif feelings found respectively are: 0.11374864130434778 0.07356385869565223\n",
      "Mean of positif and negatif feelings found respectively are: 0.1154850213980027 0.07835235378031383\n",
      "Mean of positif and negatif feelings found respectively are: 0.11539067055393577 0.07544606413994173\n",
      "Mean of positif and negatif feelings found respectively are: 0.1073305555555555 0.07520416666666668\n",
      "Mean of positif and negatif feelings found respectively are: 0.11485486211901319 0.0712989840348331\n",
      "Mean of positif and negatif feelings found respectively are: 0.10910249307479229 0.08349030470914129\n",
      "Mean of positif and negatif feelings found respectively are: 0.10925321888412029 0.07222603719599427\n",
      "Mean of positif and negatif feelings found respectively are: 0.10462119013062408 0.08747460087082727\n",
      "Mean of positif and negatif feelings found respectively are: 0.10722347949080625 0.07843422913719943\n",
      "Mean of positif and negatif feelings found respectively are: 0.11739832869080796 0.08234261838440116\n",
      "Mean of positif and negatif feelings found respectively are: 0.10647988505747133 0.07252586206896552\n",
      "Mean of positif and negatif feelings found respectively are: 0.11215418502202626 0.08059471365638775\n",
      "Mean of positif and negatif feelings found respectively are: 0.11116890080428951 0.08573324396782842\n",
      "Mean of positif and negatif feelings found respectively are: 0.10730434782608696 0.08631136044880783\n",
      "Mean of positif and negatif feelings found respectively are: 0.11381571815718164 0.07067479674796745\n",
      "Mean of positif and negatif feelings found respectively are: 0.09995154777927313 0.07635800807537015\n",
      "Mean of positif and negatif feelings found respectively are: 0.12097651933701659 0.07919475138121547\n",
      "Mean of positif and negatif feelings found respectively are: 0.1108070921985815 0.08112482269503546\n",
      "Mean of positif and negatif feelings found respectively are: 0.11309635416666679 0.07813281250000002\n",
      "Mean of positif and negatif feelings found respectively are: 0.11099363057324833 0.08196178343949045\n",
      "File finished\n",
      "Folder finished\n",
      "0.11021394688178382 0.07934325838179723\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "json_files = glob.glob(os.path.join(path, \"*.json.bz2\"))\n",
    "lista='quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences','probas', 'urls', 'phase', 'domains', 'website'\n",
    "df_prime=pd.DataFrame(columns=lista) \n",
    "\n",
    "pos=0\n",
    "posp=0\n",
    "neg=0\n",
    "negp=0\n",
    "tot=0\n",
    "totp=0\n",
    "avp=0\n",
    "avn=0\n",
    "\n",
    "def process_chunk(chunk):\n",
    "        Desc = namedtuple(\"Desc\", [\"pos\", \"neg\", \"tot\"])\n",
    "        chunk['website']=chunk['website'].astype('str')\n",
    "        immi=pd.DataFrame()\n",
    "        immi=chunk[chunk['quotation'].str.contains(\"immigrat\")]\n",
    "        immi=chunk[chunk['website'].str.contains(\"www.foxnews.com\")]\n",
    "        a,b,c = fon_nltk (immi)\n",
    "        return (Desc(a,b,c,))\n",
    "    \n",
    "for f in json_files:      \n",
    "    for chunk in pd.read_json(f, lines=True, compression='bz2', chunksize=100000, encoding='utf-8'):\n",
    "        posp,negp,totp=process_chunk(chunk)\n",
    "        pos+=posp\n",
    "        neg+=negp\n",
    "        tot+=totp\n",
    "    print('File finished')\n",
    "print('Folder finished')\n",
    "avp = pos/tot\n",
    "avn = neg/tot\n",
    "print(avp,avn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NLTK:   ny immi 2020: pos = 0.10646785272702293 neg = 0.0780937628795164\n",
    "        fox immi 2020: pos = 0.11021394688178382 neg = 0.07934325838179723 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fon_nltk (df):\n",
    "    Desc = namedtuple(\"Desc\", [\"pos\", \"neg\", \"tot\"])\n",
    "    quotes=df['quotation']\n",
    "    length= len(df)\n",
    "    pos=0\n",
    "    neg=0\n",
    "    average_pos=0\n",
    "    average_neg=0\n",
    "    for quotation in quotes:\n",
    "        result=sia.polarity_scores(quotation)\n",
    "        pos+=result[\"pos\"]\n",
    "        neg+=result[\"neg\"]\n",
    "    average_pos=pos/length\n",
    "    average_neg=neg/length\n",
    "    print('Mean of positif and negatif feelings found respectively are:', average_pos,average_neg)\n",
    "    return (Desc(pos, neg, length,))\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text2emotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fon_text2 (df):\n",
    "    Desc = namedtuple(\"Desc\", [\"fear\", \"happy\", \"angry\",\"surprise\",\"sad\",\"length\"])\n",
    "    df_quotes=df['quotation']\n",
    "    dfsize=len(df_quotes)\n",
    "    fear=0\n",
    "    happy=0\n",
    "    angry=0\n",
    "    surprise=0\n",
    "    sad=0\n",
    "    average_fear=0\n",
    "    average_happy=0\n",
    "    average_angry=0\n",
    "    average_surprise=0\n",
    "    average_sad=0\n",
    "\n",
    "    for quotation in df_quotes:\n",
    "        di=te.get_emotion(quotation)\n",
    "        fear+=di.get('Fear')\n",
    "        happy+=di.get('Happy')\n",
    "        angry+=di.get('Angry')\n",
    "        surprise+=di.get('Surprise')\n",
    "        sad+=di.get('Sad')\n",
    "    average_fear=fear/dfsize\n",
    "    average_happy=happy/dfsize\n",
    "    average_angry=angry/dfsize\n",
    "    average_surprise=surprise/dfsize\n",
    "    average_sad=sad/dfsize\n",
    "\n",
    "    print(\"fear{}, happy{}, angry{}, surprise{}, sad{}\".format(average_fear,average_happy,\n",
    "                                                               average_angry,average_surprise,average_sad))\n",
    "    return (Desc(fear,happy,angry,surprise,sad,dfsize,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear0.00826864953977195, happy0.0023258689380409405, angry0.0009496496771534548, surprise0.00486261849155104, sad0.004908297843110319\n",
      "fear0.008142602005770024, happy0.0025810550899848877, angry0.0011443879653798598, surprise0.004699477950267896, sad0.005014768512158264\n",
      "fear0.00880065943124056, happy0.0027593075971974164, angry0.0012903558181068828, surprise0.00519851627970875, sad0.005457136969363926\n",
      "fear0.008467165819480697, happy0.00275106470669048, angry0.001313023767000961, surprise0.004763016897925543, sad0.00446077757933782\n",
      "fear0.00859561752988048, happy0.0026222695425195766, angry0.0014380409396895176, surprise0.004732792966066768, sad0.004953290287127352\n",
      "fear0.00810516554471768, happy0.0024883225717818387, angry0.0011625910152493473, surprise0.0050206072262673485, sad0.004607088885835969\n",
      "fear0.007833837065530983, happy0.002146586069515043, angry0.0011818244264322022, surprise0.005018203049869488, sad0.004965998076658882\n",
      "fear0.008574323396070898, happy0.0027108806154691584, angry0.0009740348949031458, surprise0.004667536749553512, sad0.004729014974584421\n",
      "fear0.008647135595548845, happy0.0024107020195081753, angry0.0012776480285753534, surprise0.005128795164170906, sad0.005012364335760408\n",
      "fear0.009074735540596237, happy0.0022317626047533992, angry0.0012529193570545404, surprise0.004678183816458309, sad0.004556601181480974\n",
      "fear0.009611897238631676, happy0.0026954251957686495, angry0.0012759307597197417, surprise0.004927531254293172, sad0.005308078032696803\n",
      "fear0.008709300728121991, happy0.0022722901497458435, angry0.0009259513669460089, surprise0.00457308696249485, sad0.005621307871960436\n",
      "fear0.008922242066217893, happy0.0025989146860832544, angry0.0013123368594587165, surprise0.005064912762742137, sad0.005445459541145762\n",
      "fear0.008951092182992176, happy0.0027730457480423134, angry0.0011584695699958783, surprise0.0054138617942025005, sad0.0048007968127490065\n",
      "fear0.008934606401978297, happy0.0027455694463525203, angry0.001060585245225992, surprise0.004922035993955214, sad0.004674405824975959\n",
      "fear0.009381096304437423, happy0.002169940925951367, angry0.0012433026514631132, surprise0.004968402253056745, sad0.005614095342766869\n",
      "fear0.008456862206347029, happy0.002402459129001236, angry0.0009537711224069235, surprise0.0045445802994916904, sad0.0046428080780327\n",
      "fear0.008682511333974451, happy0.002412075834592665, angry0.0011890369556257728, surprise0.004890094793240833, sad0.004828273114438797\n",
      "fear0.00842595136694602, happy0.002350254155790631, angry0.0011275587305948614, surprise0.004914823464761643, sad0.004970806429454601\n",
      "fear0.008942505838714115, happy0.0026239868113751897, angry0.0011220634702569028, surprise0.005641571644456658, sad0.005250721252919357\n",
      "fear0.008308490177222143, happy0.002056601181480974, angry0.0009534276686358016, surprise0.004222764115949995, sad0.004679557631542797\n",
      "fear0.007834180519302101, happy0.0025384668223657103, angry0.001352864404451161, surprise0.00506663003159775, sad0.0046963868663277945\n",
      "fear0.008629962906992717, happy0.0024872922104684715, angry0.0008713422173375464, surprise0.004710811924714936, sad0.005100288501167744\n",
      "fear0.008481934331638968, happy0.0025099601593625486, angry0.0011752988047808762, surprise0.004430210193707927, sad0.004913793103448278\n",
      "fear0.00880065943124056, happy0.0025652562165132562, angry0.001064706690479461, surprise0.005196112103310895, sad0.004717337546366259\n",
      "fear0.009089504052754495, happy0.0024265008929798054, angry0.0012834867426844345, surprise0.004919288363786235, sad0.004897307322434406\n",
      "fear0.008362412419288359, happy0.0023784173650226686, angry0.0011670559142739384, surprise0.005175161423272426, sad0.005020607226267348\n",
      "fear0.009218986124467645, happy0.00259548014837203, angry0.0012525759032834178, surprise0.004655859321335352, sad0.0046596373128176966\n",
      "fear0.008368594587168569, happy0.0021819618079406515, angry0.0012621926088748456, surprise0.004403764253331502, sad0.0044144113202362984\n",
      "fear0.00800384668223658, happy0.0023839126253606267, angry0.0009290424508861103, surprise0.004678527270229428, sad0.004735197142464625\n",
      "fear0.008410839401016624, happy0.0022685121582634977, angry0.0012055227366396478, surprise0.00513154279433988, sad0.00487841736502267\n",
      "fear0.00860042588267619, happy0.0022516829234784995, angry0.001020401154004671, surprise0.004891125154554199, sad0.0049948481934331684\n",
      "fear0.009289394147547744, happy0.0028547877455694457, angry0.0009956724824838575, surprise0.005348262123918122, sad0.0050913587031185615\n",
      "fear0.00903352108806155, happy0.002663140541283143, angry0.0011588130237670006, surprise0.005061478225030913, sad0.0050401840912213235\n",
      "fear0.00833768374776755, happy0.002482140403901637, angry0.0010307047671383428, surprise0.004702225580436875, sad0.0048966204148921605\n",
      "fear0.008630993268306084, happy0.002341324357741448, angry0.0011433576040664924, surprise0.004775037779914826, sad0.0045947245500755594\n",
      "fear0.009125566698722356, happy0.002313161148509411, angry0.0010300178595960981, surprise0.00481110042588268, sad0.005403901634839948\n",
      "fear0.00845274076109356, happy0.0023633053990932827, angry0.001141296881439758, surprise0.005120208819892844, sad0.004577895315290565\n",
      "fear0.008560585245225998, happy0.0026154004670971296, angry0.0012299079543893387, surprise0.004834798736090122, sad0.005235952740761096\n",
      "fear0.008428012089572744, happy0.0020040527544992444, angry0.0012824563813710673, surprise0.005001717268855614, sad0.004663071850528916\n",
      "fear0.008000068690754232, happy0.002212872647341668, angry0.0012852040115400463, surprise0.0046510509685396394, sad0.0051607363648852895\n",
      "fear0.008685945871685676, happy0.002359870861382058, angry0.0009791867014699819, surprise0.0053420799560379185, sad0.004290767962632232\n",
      "fear0.00876837477675505, happy0.002597540870998764, angry0.0012656271465860689, surprise0.004865709575491139, sad0.005257933782112932\n",
      "fear0.008872784723176255, happy0.0021259788432476993, angry0.0010952740761093554, surprise0.004689517790905347, sad0.004603997801895866\n",
      "fear0.008096922654210735, happy0.002478362412419289, angry0.001118285478774557, surprise0.0048183129550762515, sad0.004667536749553514\n",
      "fear0.009438796537985995, happy0.0027321747492787484, angry0.0010702019508174196, surprise0.005515180656683612, sad0.004685052891880754\n",
      "fear0.008327723588405006, happy0.0019126940513806846, angry0.0010853139167468048, surprise0.0055962357466685, sad0.004875669734853686\n",
      "fear0.00868079406511884, happy0.0026188350048083524, angry0.0011072949580986397, surprise0.00506353894765765, sad0.005079681274900404\n",
      "fear0.008930828410495955, happy0.002284997939277373, angry0.0012213216101112784, surprise0.0052648028575353785, sad0.005262742134908643\n",
      "fear0.008458236021431515, happy0.002749347437834868, angry0.0012110179969776068, surprise0.005469157851353206, sad0.004626665750789945\n",
      "fear0.008767344415441683, happy0.00241860145624399, angry0.0011828547877455685, surprise0.005064912762742135, sad0.004666849842011265\n",
      "fear0.009350528918807529, happy0.002476301689792554, angry0.0013779365297430964, surprise0.00510750103036132, sad0.005679695013051238\n",
      "fear0.003911251545541971, happy0.0009493062233823326, angry0.0006831295507624673, surprise0.002049388652287402, sad0.002121513944223107\n",
      "File finished\n",
      "Folder finished\n",
      "0.35409723152788575 0.10009138906011313 0.047276552408007926 0.2023088467826227 0.20133170744780648\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "json_files = glob.glob(os.path.join(path, \"*.json.bz2\"))\n",
    "lista='quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences','probas', 'urls', 'phase', 'domains', 'website'\n",
    "df_prime=pd.DataFrame(columns=lista) \n",
    "\n",
    "fear=0\n",
    "fearp=0\n",
    "\n",
    "happy=0\n",
    "happyp=0\n",
    "\n",
    "angry=0\n",
    "angryp=0\n",
    "\n",
    "surprise=0\n",
    "surprisep=0\n",
    "\n",
    "sad=0\n",
    "sadp=0\n",
    "\n",
    "tot=0\n",
    "totp=0\n",
    "\n",
    "avf=0\n",
    "avh=0\n",
    "ava=0\n",
    "avs=0\n",
    "avsa=0\n",
    "\n",
    "def process_chunk(chunk):\n",
    "        Desc = namedtuple(\"Desc\", [\"fear\", \"happy\", \"angry\",\"surprise\",\"sad\",\"tot\"])\n",
    "        chunk['website']=chunk['website'].astype('str')\n",
    "        immi=pd.DataFrame()\n",
    "        immi=chunk[chunk['quotation'].str.contains(\"immigrat\")]\n",
    "        immi=chunk[chunk['website'].str.contains(\"www.foxnews.com\")]\n",
    "        a,b,c,d,e,f = fon_text2 (immi)\n",
    "        return (Desc(a,b,c,d,e,f,))\n",
    "    \n",
    "for f in json_files:      \n",
    "    for chunk in pd.read_json(f, lines=True, compression='bz2', chunksize=100000, encoding='utf-8'):\n",
    "        fearp,happyp,angryp,surprisep,sadp,totp=process_chunk(chunk)\n",
    "        fear+=fearp\n",
    "        happy+=happyp\n",
    "        angry+=angryp\n",
    "        surprise+=surprisep\n",
    "        sad+=sadp\n",
    "        tot+=totp\n",
    "    print('File finished')\n",
    "print('Folder finished')\n",
    "\n",
    "avf = fear/tot\n",
    "avh = happy/tot\n",
    "ava = angry/tot\n",
    "avs = surprise/tot\n",
    "avsa = sad/tot\n",
    "\n",
    "print(avf,avh,ava,avs,avsa)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ny: 0.3395064569308972 0.09383088336309932 0.04767413106195905 0.17941441132023636 0.194604341255667\n",
    "fox:0.35409723152788575 0.10009138906011313 0.047276552408007926 0.2023088467826227 0.20133170744780648"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
