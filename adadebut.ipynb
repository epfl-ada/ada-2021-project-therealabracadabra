{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation d'un nouveau fichier avec une nouvelle colonne pour le domaine (cf colab)\n",
    "from tld import get_tld\n",
    "\n",
    "def get_domain(url):\n",
    "    res = get_tld(url, as_object=True)\n",
    "    return res.tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pareil\n",
    "path_to_file = 'quotes-2020.json.bz2' \n",
    "path_to_out = 'quotes-2020-domains.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            urls = instance['urls'] # extracting list of links\n",
    "            domains = []\n",
    "            for url in urls:\n",
    "                tld = get_domain(url)\n",
    "                domains.append(tld)\n",
    "            instance['domains'] = domains # updating the sample with domain name\n",
    "            d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cette fois ci on rajoute une colonne avec simplement le nom du site\n",
    "path_to_file = 'quotes-2020-domains.json.bz2' \n",
    "path_to_out = '2020a.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            urls = instance['urls'] # extracting list of links\n",
    "            website = []\n",
    "            for url in urls:\n",
    "                net = urlparse(url)\n",
    "                neto=net.netloc\n",
    "                website.append(neto)\n",
    "            instance['website'] = website # updating the sample with domain name\n",
    "            d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print de cette colonne\n",
    "path_to_file = '2020a.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            print(instance['website']) # extracting list of links   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 250000 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n",
      "Processing chunk with 244449 rows\n",
      "Index(['quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences',\n",
      "       'probas', 'urls', 'phase', 'domains', 'website'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#on chun le dataset nouvellement créé\n",
    "def process_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        print(chunk.columns)\n",
    "for chunk in pd.read_json('2020a.json.bz2', lines=True, compression='bz2', chunksize=250000, encoding='utf-8'):\n",
    "    process_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['www.msn.com']</td>\n",
       "      <td>1202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['express.co.uk']</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.cheatsheet.com']</td>\n",
       "      <td>925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.bostonglobe.com']</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.foxnews.com']</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['timesofindia.indiatimes.com']</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.dailystar.co.uk']</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.telegraph.co.uk']</td>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['indianexpress.com']</td>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.thesun.co.uk']</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.thehindu.com']</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.breitbart.com']</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.forbes.com']</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.mirror.co.uk']</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.bbc.co.uk']</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.washingtonexaminer.com']</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.hindustantimes.com']</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.theguardian.com']</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['msn.com']</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.latimes.com']</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 website\n",
       "['www.msn.com']                     1202\n",
       "['express.co.uk']                   1184\n",
       "['www.cheatsheet.com']               925\n",
       "['www.bostonglobe.com']              769\n",
       "['www.foxnews.com']                  763\n",
       "['timesofindia.indiatimes.com']      745\n",
       "['www.dailystar.co.uk']              742\n",
       "['www.telegraph.co.uk']              741\n",
       "['indianexpress.com']                741\n",
       "['www.thesun.co.uk']                 730\n",
       "['www.thehindu.com']                 722\n",
       "['www.breitbart.com']                717\n",
       "['www.forbes.com']                   703\n",
       "['www.mirror.co.uk']                 642\n",
       "['www.bbc.co.uk']                    631\n",
       "['www.washingtonexaminer.com']       596\n",
       "['www.hindustantimes.com']           581\n",
       "['www.theguardian.com']              544\n",
       "['msn.com']                          539\n",
       "['www.latimes.com']                  523"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis=chunk['website'].value_counts().to_frame()\n",
    "lis.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>None</td>\n",
       "      <td>84128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>President Donald Trump</td>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>President Trump</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Kirstjen Nielsen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jennifer Anderson</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>alan tongue</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Wes Welker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Blake Hardwick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62330 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        speaker\n",
       "None                      84128\n",
       "President Donald Trump     1258\n",
       "Bernie Sanders              595\n",
       "Joe Biden                   581\n",
       "President Trump             546\n",
       "...                         ...\n",
       "Kirstjen Nielsen              1\n",
       "Jennifer Anderson             1\n",
       "alan tongue                   1\n",
       "Wes Welker                    1\n",
       "Blake Hardwick                1\n",
       "\n",
       "[62330 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk['speaker'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk['website']=chunk['website'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation d'un dataframe pour quelques journaux\n",
    "fox=chunk[chunk['website'].str.contains(\"www.foxnews.com\")]\n",
    "ny=chunk[chunk['website'].str.contains(\"www.nytimes.com\")]\n",
    "brei=chunk[chunk['website'].str.contains(\"www.breitbart.com\")]\n",
    "cnn=chunk[chunk['website'].str.contains(\"cnn.com\")]\n",
    "guard=chunk[chunk['website'].str.contains(\"www.theguardian.com\")]\n",
    "slate=chunk[chunk['website'].str.contains(\"slate.com\")]\n",
    "buzz=chunk[chunk['website'].str.contains(\"buzzfeed.com\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_quotes=fox['quotation']\n",
    "ny_quotes=ny['quotation']\n",
    "brei_quotes=brei['quotation']\n",
    "cnn_quotes=cnn['quotation']\n",
    "guard_quotes=guard['quotation']\n",
    "slate_quotes=slate['quotation']\n",
    "buzz_quotes=buzz['quotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1787, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1284, 11)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3708, 11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1446, 11)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(687, 11)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123, 11)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 11)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buzz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11182372691661988 0.07998601007274761\n"
     ]
    }
   ],
   "source": [
    "#analyse de sentiments positif et negatif pour les journaux précédemment choisis\n",
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in fox_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/1787\n",
    "average_neg=neg/1787\n",
    "print(average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10962928348909654 0.07516510903426793\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in ny_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/1284\n",
    "average_neg=neg/1284\n",
    "print(average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12019363538295594 0.0813460086299893\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in brei_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/3708\n",
    "average_neg=neg/3708\n",
    "print(average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.113540110650069 0.06954356846473027\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in cnn_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/1446\n",
    "average_neg=neg/1446\n",
    "print(average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11037845705967979 0.07798107714701595\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in guard_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/687\n",
    "average_neg=neg/687\n",
    "print(average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09873170731707316 0.07367479674796752\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in slate_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/123\n",
    "average_neg=neg/123\n",
    "print(average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1609749103942653 0.06458064516129029\n"
     ]
    }
   ],
   "source": [
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in buzz_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/279\n",
    "average_neg=neg/279\n",
    "print(average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
