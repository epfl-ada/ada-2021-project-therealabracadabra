{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Step: Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching for Missing Values\n",
    "def MissingValuesFilter_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "\n",
    "        #Gives True as a result, if the lines where NaN are presents are empty \n",
    "        #(meaning there are no NaNs):\n",
    "        missing_nan = pd.DataFrame(np.where(chunk.isnull().any(axis=1))).empty\n",
    "        #Gives True as a result if the lignes and corresponding columns where \n",
    "        #zeros, '' and None are found are empty:\n",
    "        missing_zeros = pd.DataFrame(np.where(chunk==0)).empty \n",
    "        missing_space = pd.DataFrame(np.where(chunk=='')).empty\n",
    "        missing_none = pd.DataFrame(np.where(chunk==None)).empty\n",
    "\n",
    "        print('This chunk does not contain missing values')\n",
    "        print(missing_nan, missing_zeros, missing_space, missing_none) \n",
    "\n",
    "PATH = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2'\n",
    "\n",
    "for chunk in pd.read_json(PATH, lines=True, compression='bz2', chunksize=250000):\n",
    "    MissingValuesFilter_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if Number of Occurrences is coherent\n",
    "def NumOccurrencesFilter_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        size = len(chunk)\n",
    "        #To have a number of occurrences equal zero has no meaning, so here it checks if this data set \n",
    "        if chunk['numOccurrences'].all() == np.array(size*[0]).all():\n",
    "          print('Number of Occurrences impossible')\n",
    "        else :\n",
    "          print('Number of Occurences posible')\n",
    "\n",
    "PATH = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2'\n",
    "\n",
    "for chunk in pd.read_json(PATH, lines=True, compression='bz2', chunksize=250000):\n",
    "    NumOccurrencesFilter_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if the Highest Probability Corresponds to the Autor of the quotation\n",
    "def ProbasFilter_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "  \n",
    "        if chunk['probas'].str[0].str[1].astype(float).all() < np.array(size*[0.5]).all():\n",
    "          print('Probability of Autors of quotation too low')\n",
    "        else :\n",
    "          print('Probability of Autors higher than 50%')\n",
    "\n",
    "PATH = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2'\n",
    "\n",
    "for chunk in pd.read_json(PATH, lines=True, compression='bz2', chunksize=250000):\n",
    "    ProbasFilter_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if the Autor is the one with the highest probability of having generated the quotation\n",
    "def ProbasFilter_chunk(chunk):\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "  \n",
    "        if chunk['speaker'].all() != chunk['probas'].str[0].str[0].all():\n",
    "          print('The Autor does not correspond to the high probability Autor')\n",
    "        else :\n",
    "          print('The Autor has a high probability of having generated the quotation')\n",
    "\n",
    "PATH = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2'\n",
    "\n",
    "for chunk in pd.read_json(PATH, lines=True, compression='bz2', chunksize=250000):\n",
    "    ProbasFilter_chunk(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the Data with Streaming \n",
    "\n",
    "#THIS PART IS ONLY NEEDED IF THE TESTS FROM BEFORE SHOWED PROBLEMS\n",
    "\n",
    "#If the above checks show that there is a missing values/incoherence or problem\n",
    "#in the data for any year, this code could remove the above mentionned problems \n",
    "#and creates a new file with the filtered data\n",
    "path_to_file = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2' \n",
    "path_to_out = '/content/drive/MyDrive/ADA_2021/quotes-2020-filtered.json.bz2'\n",
    "\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            #Removes '', None, 0 and False from the quotation in instance:\n",
    "            if any([bool(instance['quotation']) == True for quotation in instance]): \n",
    "              #If the probability the quotation comes from a speaker is lower \n",
    "              #than 50% than we discard this probability (as not high enough):\n",
    "              if float(instance['probas'][0][1]) > 0.5: \n",
    "                #Removes the lines that are not coherents with respect to the speakers probabilities\n",
    "                if instance['speaker'] == instance['probas'][0][0]: \n",
    "                  if instance['numOccurrences'] != 0: #Removes incoherent occurences\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creation of the DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Cleanning and Storage of the Data for One Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning of the 2020 Data\n",
    "df = pd.DataFrame() #Defining the DataFrame that will contain the wanted chucks\n",
    "\n",
    "def process_chunk(chunk, df):\n",
    "        df_1 = pd.DataFrame() #Initialising intermediate DataFrame\n",
    "        df_2 = pd.DataFrame() #Initialising intermediate DataFrame\n",
    "\n",
    "        # Calculates duplicated rows in the chunk\n",
    "        num_duplicates = len(chunk[chunk.duplicated(subset=[\"quoteID\"])])\n",
    "        print(\"There were {} duplicated rows\".format(num_duplicates))\n",
    "\n",
    "        # Removes duplicated rows\n",
    "        df = chunk.drop_duplicates(subset=[\"quoteID\"], keep=\"first\")\n",
    "\n",
    "        df = chunk.drop(['quoteID', 'qids', 'numOccurrences', 'probas', 'urls', 'phase', 'domains'], axis=1) #axis=1 for columns\n",
    "        \n",
    "        df['website'] = df['website'].astype('str') #Converting to string to be able to use str.contains \n",
    "        df_1 = df[df['website'].str.contains('foxnews')] #Creating a DataFrame containing only foxnews\n",
    "        df_2 = df[df['website'].str.contains('nytimes')] #Creating a DataFrame containing only nytimes\n",
    "        df = pd.concat([df_1, df_2])\n",
    "\n",
    "        print(df.shape)\n",
    "        print(f'Processing chunk with {len(chunk)} rows')\n",
    "        return df\n",
    "        \n",
    "\n",
    "for chunk in pd.read_json('/content/drive/MyDrive/ADA_2021/2020a.json.bz2', lines=True, compression='bz2', chunksize=250000):\n",
    "    df = process_chunk(chunk, df)\n",
    "    #df.to_pickle('/content/drive/MyDrive/ADA_2021/SelectedData.pkl') #Transforming the df DataFrame into a pickle file\n",
    "    with open('ReadyToUseData.pickle', 'ab') as file: #Opening pickle file in append mode, to keep the previous data of the file\n",
    "        pickle.dump(df, file) #Adding the new chunks to the pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Dealing with the entire Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of a new file with a new column for the domain (cf colab)\n",
    "from tld import get_tld\n",
    "\n",
    "def get_domain(url):\n",
    "    res = get_tld(url, as_object=True)\n",
    "    return res.tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of a file with additional columns: the domain and the name of the intenet website\n",
    "path_to_file = 'quotes-2017.json.bz2' \n",
    "path_to_out = '2017a.json.bz2'\n",
    "\n",
    "with bz2.open(path_to_file, 'rb') as s_file:\n",
    "    with bz2.open(path_to_out, 'wb') as d_file:\n",
    "        for instance in s_file:\n",
    "            instance = json.loads(instance) # loading a sample\n",
    "            urls = instance['urls'] # extracting list of links\n",
    "            domains = []\n",
    "            website = []\n",
    "            for url in urls:\n",
    "                tld = get_domain(url)\n",
    "                domains.append(tld)\n",
    "                net = urlparse(url)\n",
    "                neto=net.netloc\n",
    "                website.append(neto)\n",
    "            instance['domains'] = domains # updating the sample with domain name\n",
    "            instance['website'] = website\n",
    "            d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing in the new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the DataFrame over the 5 years period, with the quotations containing a specific word. Here \"immigrat\"is chosen. \n",
    "path = os.getcwd()\n",
    "json_files = glob.glob(os.path.join(path, \"*.json.bz2\"))\n",
    "lista='quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences','probas', 'urls', 'phase', 'domains', 'website'\n",
    "df2=pd.DataFrame(columns=lista) \n",
    "\n",
    "def process_chunk(chunk,df2):\n",
    "        immi=pd.DataFrame()\n",
    "        immi=chunk[chunk['quotation'].str.contains(\"immigrat\")]\n",
    "        df2 = df2.append(immi)\n",
    "        return df2\n",
    "    \n",
    "for f in json_files:      \n",
    "    for chunk in pd.read_json(f, lines=True, compression='bz2', chunksize=100000, encoding='utf-8'):\n",
    "        df2=process_chunk(chunk,df2)\n",
    "    print('File finished')\n",
    "print('Folder finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping dupliactes, if any\n",
    "duplica2 = df2.drop_duplicates(subset=[\"quoteID\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation du dataframe à partir des 5 années avec les citations d'un site en particulier\n",
    "path = os.getcwd()\n",
    "json_files = glob.glob(os.path.join(path, \"*.json.bz2\"))\n",
    "lista='quoteID', 'quotation', 'speaker', 'qids', 'date', 'numOccurrences','probas', 'urls', 'phase', 'domains', 'website'\n",
    "df3=pd.DataFrame(columns=lista) \n",
    "\n",
    "def process_chunk(chunk,df3):\n",
    "        chunk['website']=chunk['website'].astype('str')\n",
    "        journal=pd.DataFrame()\n",
    "        journal=chunk[chunk['website'].str.contains(\"www.foxnews.com\")]\n",
    "        df3 = df3.append(journal)\n",
    "        return df3\n",
    "    \n",
    "for f in json_files:      \n",
    "    for chunk in pd.read_json(f, lines=True, compression='bz2', chunksize=25000, encoding='utf-8'):\n",
    "        df3=process_chunk(chunk,df3)\n",
    "    print('File finished')\n",
    "print('Folder finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3456, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping dupliactes, if any\n",
    "duplica3 = df3.drop_duplicates(subset=[\"quoteID\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>['www.msn.com']</td>\n",
       "      <td>1202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['express.co.uk']</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.cheatsheet.com']</td>\n",
       "      <td>925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.bostonglobe.com']</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.foxnews.com']</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['timesofindia.indiatimes.com']</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.dailystar.co.uk']</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.telegraph.co.uk']</td>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['indianexpress.com']</td>\n",
       "      <td>741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.thesun.co.uk']</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.thehindu.com']</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.breitbart.com']</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.forbes.com']</td>\n",
       "      <td>703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.mirror.co.uk']</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.bbc.co.uk']</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.washingtonexaminer.com']</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.hindustantimes.com']</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.theguardian.com']</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['msn.com']</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>['www.latimes.com']</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 website\n",
       "['www.msn.com']                     1202\n",
       "['express.co.uk']                   1184\n",
       "['www.cheatsheet.com']               925\n",
       "['www.bostonglobe.com']              769\n",
       "['www.foxnews.com']                  763\n",
       "['timesofindia.indiatimes.com']      745\n",
       "['www.dailystar.co.uk']              742\n",
       "['www.telegraph.co.uk']              741\n",
       "['indianexpress.com']                741\n",
       "['www.thesun.co.uk']                 730\n",
       "['www.thehindu.com']                 722\n",
       "['www.breitbart.com']                717\n",
       "['www.forbes.com']                   703\n",
       "['www.mirror.co.uk']                 642\n",
       "['www.bbc.co.uk']                    631\n",
       "['www.washingtonexaminer.com']       596\n",
       "['www.hindustantimes.com']           581\n",
       "['www.theguardian.com']              544\n",
       "['msn.com']                          539\n",
       "['www.latimes.com']                  523"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking that the column was well added\n",
    "lis=df2['website'].value_counts().to_frame()\n",
    "lis.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>None</td>\n",
       "      <td>84128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>President Donald Trump</td>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>President Trump</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Kirstjen Nielsen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Jennifer Anderson</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>alan tongue</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Wes Welker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Blake Hardwick</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62330 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        speaker\n",
       "None                      84128\n",
       "President Donald Trump     1258\n",
       "Bernie Sanders              595\n",
       "Joe Biden                   581\n",
       "President Trump             546\n",
       "...                         ...\n",
       "Kirstjen Nielsen              1\n",
       "Jennifer Anderson             1\n",
       "alan tongue                   1\n",
       "Wes Welker                    1\n",
       "Blake Hardwick                1\n",
       "\n",
       "[62330 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualisation of higher quoted speakers\n",
    "lis2=df3['speaker'].value_counts().to_frame()\n",
    "lis2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the website column type to string\n",
    "df2['website']=df2['website'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of DataFrames for some journals\n",
    "fox=df2[df2['website'].str.contains(\"www.foxnews.com\")]\n",
    "ny=df2[df2['website'].str.contains(\"www.nytimes.com\")]\n",
    "brei=df2[df2['website'].str.contains(\"www.breitbart.com\")]\n",
    "cnn=df2[df2['website'].str.contains(\"cnn.com\")]\n",
    "guard=df2[df2['website'].str.contains(\"www.theguardian.com\")]\n",
    "slate=df2[df2['website'].str.contains(\"slate.com\")]\n",
    "buzz=df2[df2['website'].str.contains(\"buzzfeed.com\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_quotes=fox['quotation']\n",
    "ny_quotes=ny['quotation']\n",
    "brei_quotes=brei['quotation']\n",
    "cnn_quotes=cnn['quotation']\n",
    "guard_quotes=guard['quotation']\n",
    "slate_quotes=slate['quotation']\n",
    "buzz_quotes=buzz['quotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1787, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fox.shape\n",
    "ny.shape\n",
    "brei.shape\n",
    "cnn.shape\n",
    "guard.shape\n",
    "slate.shape\n",
    "buzz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11182372691661988 0.07998601007274761\n"
     ]
    }
   ],
   "source": [
    "#Sentimental Analysis for positif and negatif feelings for the previously chosen articles with df2\n",
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in fox_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/1787\n",
    "average_neg=neg/1787\n",
    "print('Mean of positif and negatif feelings found respectively', average_pos,average_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_quotes=df3['quotation']\n",
    "df3size=df3_quotes.shape(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09833275462962988 0.08480266203703696\n"
     ]
    }
   ],
   "source": [
    "#Sentimental Analysis for positif and negatif feelings for the previously chosen articles with df3\n",
    "pos=0\n",
    "neg=0\n",
    "average_pos=0\n",
    "average_neg=0\n",
    "for quotation in df3_quotes:\n",
    "    result=sia.polarity_scores(quotation)\n",
    "    pos=pos+result[\"pos\"]\n",
    "    neg=neg+result[\"neg\"]\n",
    "average_pos=pos/df3size\n",
    "average_neg=neg/df3size\n",
    "print('Mean of positif and negatif feelings found respectively',average_pos,average_neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
